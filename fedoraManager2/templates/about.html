{% include 'header.html' %}
<h2>About</h2>

<p>Wayne State University Library's "Ouroboros" came from a need to manage and provide access to our new digital collections infrastructure.  With <a href="http://www.fedora-commons.org/">Fedora Commons</a> as our digital object repository, <a href="http://lucene.apache.org/solr/">Solr</a> as our primary search and access metadata store, and a combination of PHP / HTML / CSS / JS for our front-end, we have "glued" together disparate back-end systems with what we are calling "Ouroboros".  At least for now.</p>

<p>The metaphor finds traction in the genesis and behavior of this back-end infrastructure.  We began with one-off python scripts, slightly more complex PHP API's to return metadata to our front-end, even some early attempts at combining utilities.  But over time we ended up with overlapping or confusing functionality in one-off scripts, front-end utilities with clumsy HTTP interfaces and poor security, a highly functional but isolated API for the front-end, among other problems.  It became apparent we needed to start coralling these things in one place.</p>

<p>The first intrepid attempt was something called "fedClerk".  fedClerk brought a formerly PHP based API into a Python server framework, specifically, <a href="https://twistedmatrix.com/trac/">Twisted</a>.  We chose Twisted as it deals exceptionally well with Asynchronous requests and tasks; if one API call started taking an inordinate amount of time, it wouldn't prevent other users from successfully communicating with the server.  We moved another stand-alone application, "FOXML2Solr", into this framework as well.  FOXML2Solr's primary task was to index metadata from objects in Fedora Commons into Solr for search.  To round it out, we added an small, but effective Python based image server to resize and rotate large images from Fedora for the front-end interface.  This worked well, and helped us launch our public front-end, but its overall organization was indicative of the way it was created: one-off utilities cobbled together under one server.  With our primary utility for editing materials, in bluk, in Fedora Commons also coming online, it seemed like an opportune time to reorganize them in something a bit more sensical.</p>

<p>So why Ouroboros?  First of all, the snake metaphor was nice seeing as the <em>vast</em> majority of our back-end scripts and utilities are written in Python.  Second, Ouroboros successfully chains togethers functions and functionality that was formerly iggnorant of one another.  For example, when we modify a MODS datastream in Fedora a series of events are triggered.  1) The datastream is altered.  2) A derivative Dublin Core (DC) datastream is updated.  3) FOXML2Solr fires, indexing this item in Solr, and performing secondary actions where necessary (e.g. indexing full-text from ebook objets).  But perhaps most importantly, Ouroboros, runs all time or CPU intensive tasks through the Python's distributed task queue, <a href="http://www.celeryproject.org/">Celery</a>.  Celery allows us to throttle large, bulk edits or actions so that they do not negatively effect the front-end.</p>

<p>Ouroboros has been beneficial for reasons additional to asynchronous task execution.To name a few:
	<ul>
		<li>as changes to existing functionality are needed, or new ones determined, it allows for a more organized and sustainable way to manage those processes;</li>
		<li>it allows for much more granular control over security and access - purge operations in Fedora require two users to be logged in, student workers might have access to replacable, derivative datastreams only in Fedora Commons, IP restriction, etc;</li>
		<li>provides a more graphical web-interface for work formerly delegated to hard to use terminal scripts;</li>
		<li>runs under a single server instance, easier to monitor, easier to develop, easier to manage, easier to operate;</li>
		<li>is otherwise pretty sweet;</li>
	</ul>
</p>

<p>And now, the elephant in the room: <em>Why not use one of the other Fedora / Solr stacks out there?  Dost thou reinvent thy wheel?</em></p>

<p>Two <em>very</em> good digital collections infastructures come to mind when thinking about Fedora Commons / Solr stacks: <a href="http://islandora.ca/">Islandora</a> and the <a href="http://projecthydra.org/">Hydra Project</a>.  In our early planning and research phases, both seemed to be very viable options, and they have proven to be for many other institutions.  But for our purposes neither quite fit the bill.  We knew we wanted Fedora Commons as our backend digital object repository, Solr for search (and some metadata), but we wanted to keep our front-end very light, HTML / CSS / JS only if possible.</p>

<p>Islandora had Drupal as a "glue" behind the scenes, uniting the management back-end and the access / discovery front-end.  Hydra is built around the Ruby and the Rails framework, something we had no expertise in-house with.  What prompted and made possible developing a digital collections infrastructure included the following:
	<ul>
		<li>An pre-existing, reliable, functional digital collections platform already in place!  This cannot be understated.  WSU Library has been using DLXS for years and it continues to faithfully serve our users.</li>
		<li>An interest and moderate amount of comfort with Python as our de-facto back-end language to "glue" systems together.</li>
		<li>An <em>enormously</em> supportive and knowledgable community of digital preservation practioners, making freely available their tools, utilities, language libraries, tips, and advice.  Leveraging the work of others has been central and fundamental to the success of our platform.  Envisioned more as a network of micro-services, our platform is really the tying, tethering, routing, and manipulation of information through tools and systems already built by people far more skilled.</li>
		<li>The crucial support of our colleagues and library, and foolish naivety.</li>
	</ul>
</p>

<h2>Presentations / Posters / Additional Information:</h2>
<ul>
	<li>Put them here...</li>
</ul>